{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CNN2019 - [Class Exercise 01] - b. Linear Classification 1301160098.ipynb","version":"0.3.2","provenance":[{"file_id":"1K9RFMILtglAh0Lx8znQVVjElOhuxDsy4","timestamp":1567128607210},{"file_id":"19u7tZ6QNiPHMbrOwoYDO6Ff8U8Z4Z25F","timestamp":1567089353494}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"246px"},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y4GgVIEOx63V"},"source":["![title](https://image.ibb.co/erDntK/logo2018.png)\n","\n","---\n","# [Class Exercise] Linear Classification \n","\n","In this exercise you will practice a simple Linear Classification and its multiclass loss, \n","including:\n","* implement simple steps and understand the basic Linear Classification pipeline, \n","* implement Softmax and Multiclass SVM loss"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UCWk4vo89HDQ"},"source":["---\n","## Simple Dataset\n","\n","We use a simple case with\n","* 4 data `x` of 8 dimension, \n","* 3 class target classification, \n","thus, we have a weight parameter `W` with the size of (8,3) and bias `b` of size (3,)\n","\n","![Linear Classifier](https://image.ibb.co/iCvLL9/01.png)"]},{"cell_type":"code","metadata":{"id":"M6K0M_DL9HDU","colab_type":"code","colab":{}},"source":["import numpy as np\n","np.set_printoptions(precision=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUGU_hgp9HDe","colab_type":"code","colab":{}},"source":["def simple_random(size, seed):\n","    np.random.seed(seed)\n","    return np.random.randint(20,size=size)/10-1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-t3wBvTZ9HDm","colab_type":"code","colab":{}},"source":["n_dim = 8\n","n_data = 4\n","n_class = 3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGZ2c0QS9HDs","colab_type":"code","outputId":"0d538c52-a886-4944-cb78-8b6d61363717","executionInfo":{"status":"ok","timestamp":1567089406891,"user_tz":-420,"elapsed":772,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["X = simple_random((n_data,n_dim),1)\n","print(X)\n","print('shape=',X.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[-0.5  0.1  0.2 -0.2 -0.1  0.1 -0.5  0.5]\n"," [-1.   0.6 -0.9  0.2 -0.3  0.3 -0.4  0.8]\n"," [-0.5  0.8  0.1  0.   0.4  0.8 -0.6 -0.1]\n"," [ 0.7 -1.   0.3 -0.1 -0.1 -0.3 -0.9 -1. ]]\n","shape= (4, 8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zCkxg54_9HDy","colab_type":"code","outputId":"ea054603-84ed-49b2-cd7f-5e5ed955a731","executionInfo":{"status":"ok","timestamp":1567089411269,"user_tz":-420,"elapsed":783,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["W = simple_random((n_class, n_dim),2)\n","print(W)\n","print('shape=',W.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[-0.2  0.5  0.3 -0.2  0.1  0.8  0.1 -0.2]\n"," [-0.3 -0.8  0.7  0.1  0.5 -0.5 -0.3 -0.7]\n"," [-0.4 -0.6  0.   0.1  0.9 -0.3 -0.4  0. ]]\n","shape= (3, 8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TBPnzOBe9HEB","colab_type":"code","outputId":"6242b6dc-469e-4f20-b8e1-53c033a47a28","executionInfo":{"status":"ok","timestamp":1567089413478,"user_tz":-420,"elapsed":777,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["np.random.seed(25)\n","b = simple_random((n_class,1),3)\n","print(b)\n","print('shape=',b.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0. ]\n"," [-0.7]\n"," [-0.2]]\n","shape= (3, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pD-MzwsK9HEI","colab_type":"text"},"source":["## Linear Function\n","\n","In many tutorials available on the Internet, you may find that they have different mathematical formulation for the forward (linear) function. \n","\n","But you should know that basically, depending on how you shape the matrices, it's all the same\n","\n","Below is the example from $WX'+b$ formulation. You should notice that using $XW'+b$ formulation should result the same."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"07GXoAyV9HEK","colab_type":"code","outputId":"56039a87-2646-4b53-fba7-80bd08bf76a2","executionInfo":{"status":"ok","timestamp":1567089420343,"user_tz":-420,"elapsed":1488,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["scores1 = W.dot(X.T) + b\n","print('scores =')\n","print(scores1)\n","print('shape=',scores1.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["scores =\n","[[ 0.17  0.2   1.17 -0.67]\n"," [-0.81 -2.23 -1.07  1.16]\n"," [ 0.   -0.34 -0.12  0.47]]\n","shape= (3, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9GrBk6A39HET","colab_type":"code","outputId":"fae276d2-72aa-42cb-fc55-f7970a9e06f0","executionInfo":{"status":"ok","timestamp":1567089423093,"user_tz":-420,"elapsed":772,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["scores = X.dot(W.T) + b.T\n","print('scores =')\n","print(scores.T)\n","print('shape=',scores.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["scores =\n","[[ 0.17  0.2   1.17 -0.67]\n"," [-0.81 -2.23 -1.07  1.16]\n"," [ 0.   -0.34 -0.12  0.47]]\n","shape= (4, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xmbOfJLT9HEZ","colab_type":"text"},"source":["# Multiclass Loss Function\n","\n","For multiclass classification problem, at the end of the system/network, there should be some activation/scoring function head to determine the classification. Then from the activation, we can calculate the loss/gradient to propagate back to the entire network.\n","\n","There are two popular loss functions for multiclass classification problem:\n","* Softmax Loss or Categorical Cross-entropy Loss\n","* SVM Loss or Hinge Loss\n","\n","Let's say from our previous three inputs, the the targets are as follow"]},{"cell_type":"code","metadata":{"id":"N5oZxxUe9HEa","colab_type":"code","outputId":"df9d9104-e803-448e-9aca-b7b148c0a230","executionInfo":{"status":"ok","timestamp":1567089426691,"user_tz":-420,"elapsed":1432,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["y = np.array([0, 1, 2, 1])\n","\n","print(scores)\n","print('\\n target:',y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.17 -0.81  0.  ]\n"," [ 0.2  -2.23 -0.34]\n"," [ 1.17 -1.07 -0.12]\n"," [-0.67  1.16  0.47]]\n","\n"," target: [0 1 2 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tsMsyk_D9HEf","colab_type":"text"},"source":["## Multiclass SVM Loss\n","\n","In Multiclass SVM Loss, there is no Scoring function. So we can go stright calculate its loss.\n","\n","First, get the score on the actual class (target class)"]},{"cell_type":"code","metadata":{"id":"AGEx4u-X9HEh","colab_type":"code","outputId":"1b2ca352-fc00-4aa0-e5ff-53e656b9b7c6","executionInfo":{"status":"ok","timestamp":1567089430262,"user_tz":-420,"elapsed":1268,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["img = 0\n","\n","print('score image',img,'      =', scores[img])\n","print('score on true class =', scores[img, y[img]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["score image 0       = [ 0.17 -0.81  0.  ]\n","score on true class = 0.17000000000000004\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QuVFIATj9HEm","colab_type":"text"},"source":["Then subtract the current score with the actual class score. For SVM, margin 1 is added to keep the actual class loss positive (=1)"]},{"cell_type":"code","metadata":{"id":"-UABFXar9HEn","colab_type":"code","outputId":"f22af588-cd85-406c-e747-27d0029af019","executionInfo":{"status":"ok","timestamp":1567089432757,"user_tz":-420,"elapsed":792,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print('(score image',img,') minus (score on true class) =', scores[img]-scores[img, y[img]])\n","print('margin is added by 1                         =', scores[img]-scores[img, y[img]]+1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(score image 0 ) minus (score on true class) = [ 0.   -0.98 -0.17]\n","margin is added by 1                         = [1.   0.02 0.83]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sueCa4bV9HEr","colab_type":"text"},"source":["Remove the negative loss"]},{"cell_type":"code","metadata":{"id":"kRx9aU7E9HEt","colab_type":"code","outputId":"e5f41f3b-5100-4e38-bb2f-ceed2aaa827c","executionInfo":{"status":"ok","timestamp":1567089435462,"user_tz":-420,"elapsed":768,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["margin = scores[img]-scores[img, y[img]] + 1\n","print('remove all negative loss',img, '=', np.maximum(0, margin ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["remove all negative loss 0 = [1.   0.02 0.83]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0cyAO6kJ9HEw","colab_type":"text"},"source":["Lastly, sum over all class loss and subtract by 1 for target (from margin)"]},{"cell_type":"code","metadata":{"id":"yYxjgH_79HEy","colab_type":"code","outputId":"482bc317-6d95-4a89-de7b-57cebb7c9f28","executionInfo":{"status":"ok","timestamp":1567089437599,"user_tz":-420,"elapsed":811,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["img = 0\n","\n","margin = scores[img]-scores[img, y[img]] + 1\n","losses_i = np.maximum(0, margin)\n","print('loss of example',img, '(Li) is the sum of it, minus 1 (for target) =', np.sum(losses_i) - 1 )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loss of example 0 (Li) is the sum of it, minus 1 (for target) = 0.8500000000000001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GMFkslzm9HE1","colab_type":"text"},"source":["SVM Loss is the average of loss over all examples (data)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"blvt59kw9HE2","colab_type":"code","outputId":"0460e449-51f7-412f-fb49-4394cba2ca3e","executionInfo":{"status":"ok","timestamp":1567089440575,"user_tz":-420,"elapsed":794,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["Loss_svm = []\n","\n","for img in range(n_data):\n","    margin = scores[img]-scores[img, y[img]] + 1\n","    losses_i = np.maximum(0, margin)\n","    L_i = np.sum(losses_i) - 1\n","    print('SVM Loss for data',img,':',L_i)\n","    Loss_svm.append(L_i)\n","\n","Loss_svm = np.array(Loss_svm)\n","print('\\nHinge Loss or Multiclass SVM Loss is the average of all example losses')\n","print('SVM Loss (avg) =', np.mean(Loss_svm))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM Loss for data 0 : 0.8500000000000001\n","SVM Loss for data 1 : 6.32\n","SVM Loss for data 2 : 2.34\n","SVM Loss for data 3 : 0.31000000000000005\n","\n","Hinge Loss or Multiclass SVM Loss is the average of all example losses\n","SVM Loss (avg) = 2.455\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqTokoXR9HE6","colab_type":"text"},"source":["---\n","## Softmax Loss\n","In Softmax Loss, there are two steps. First we calculate the score, then the loss. \n","![Softmax Loss](https://image.ibb.co/msQy7p/03.png)\n"]},{"cell_type":"code","metadata":{"id":"B7fMmg_u9HE7","colab_type":"code","outputId":"c1545cd9-0e69-44a5-94c3-e85368b0b316","executionInfo":{"status":"ok","timestamp":1567089452187,"user_tz":-420,"elapsed":772,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print(scores)\n","print('shape=',scores.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.17 -0.81  0.  ]\n"," [ 0.2  -2.23 -0.34]\n"," [ 1.17 -1.07 -0.12]\n"," [-0.67  1.16  0.47]]\n","shape= (4, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XtqXHcPH9HE-","colab_type":"text"},"source":["### Softmax Score\n","Softmax score will normalize the output into normalized log-probability distribution.\n","\n","First we calculate the exponent of output scores, to get the unnormalized log probability"]},{"cell_type":"code","metadata":{"id":"hx80LZVp9HE_","colab_type":"code","outputId":"9d88f2f9-bbb3-402e-a348-2a70e8a81fcd","executionInfo":{"status":"ok","timestamp":1567089455807,"user_tz":-420,"elapsed":807,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["e_scores = np.exp(scores)\n","print(e_scores)\n","print('shape=',e_scores.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1.19 0.44 1.  ]\n"," [1.22 0.11 0.71]\n"," [3.22 0.34 0.89]\n"," [0.51 3.19 1.6 ]]\n","shape= (4, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xDmsqp_G9HFC","colab_type":"text"},"source":["sum over class"]},{"cell_type":"code","metadata":{"id":"-UO64SI_9HFD","colab_type":"code","outputId":"9973f945-b444-4f9d-c8b3-00461872ab42","executionInfo":{"status":"ok","timestamp":1567089460231,"user_tz":-420,"elapsed":792,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sum_e_score = np.sum(e_scores, axis=1, keepdims = True)\n","print(sum_e_score.T)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[2.63 2.04 4.45 5.3 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UDjQlE6b9HFI","colab_type":"text"},"source":["Divide the score to get the normalized log probabilities"]},{"cell_type":"code","metadata":{"id":"PRGPII4e9HFI","colab_type":"code","outputId":"01b1d5eb-cd68-405c-95b7-0b1ff6f2e105","executionInfo":{"status":"ok","timestamp":1567089462742,"user_tz":-420,"elapsed":769,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["norm_log_prob = e_scores / sum_e_score\n","print(norm_log_prob)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.45 0.17 0.38]\n"," [0.6  0.05 0.35]\n"," [0.72 0.08 0.2 ]\n"," [0.1  0.6  0.3 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R9Xm8Zeh9HFK","colab_type":"text"},"source":["Note that now, sum over all class for each data is equal to 1. The score now better represents the classification confidence to a class."]},{"cell_type":"code","metadata":{"id":"fYXepDuf9HFL","colab_type":"code","outputId":"1dd4b46e-f253-4fe8-b7ab-5d1de2166559","executionInfo":{"status":"ok","timestamp":1567089465041,"user_tz":-420,"elapsed":763,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["img = 0\n","\n","print('probability over all classes on image', img, '      =', norm_log_prob[img])\n","print('total probability over all classes on image', img, '=', np.sum(norm_log_prob[img]))\n","print('this is the softmax score')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["probability over all classes on image 0       = [0.45 0.17 0.38]\n","total probability over all classes on image 0 = 1.0\n","this is the softmax score\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NqZtonkM9HFO","colab_type":"text"},"source":["### Categorical Crossentropy Loss\n","\n","To calculate the Softmax loss, also called categorical crossentropy, calculate the minus log of the score\n","\n","we can use the base-10 log"]},{"cell_type":"code","metadata":{"id":"PVmRZP1_9HFP","colab_type":"code","outputId":"3a3819eb-2da2-4866-c9a9-d39e289e7ed3","executionInfo":{"status":"ok","timestamp":1567089467077,"user_tz":-420,"elapsed":768,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print('log10 loss')\n","loss_i = -np.log10(norm_log_prob)\n","print(loss_i)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["log10 loss\n","[[0.35 0.77 0.42]\n"," [0.22 1.28 0.46]\n"," [0.14 1.11 0.7 ]\n"," [1.02 0.22 0.52]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S4ugdp1o9HFR","colab_type":"text"},"source":["or use natural log"]},{"cell_type":"code","metadata":{"id":"yTcbWmPh9HFS","colab_type":"code","outputId":"59f164f6-9d1c-4dfd-8517-4a3d41956ebe","executionInfo":{"status":"ok","timestamp":1567089469430,"user_tz":-420,"elapsed":518,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["loss_i_natural = -np.log(norm_log_prob)\n","print('natural log loss')\n","print(loss_i_natural)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["natural log loss\n","[[0.8  1.78 0.97]\n"," [0.51 2.94 1.05]\n"," [0.32 2.56 1.61]\n"," [2.34 0.51 1.2 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"311EU1L69HFW","colab_type":"text"},"source":["Like SVM Loss, Softmax loss is the average of all example (data)"]},{"cell_type":"code","metadata":{"id":"hIu0V1c69HFY","colab_type":"code","outputId":"cdda6ddc-aa02-4522-884e-9d911e4a302d","executionInfo":{"status":"ok","timestamp":1567089471879,"user_tz":-420,"elapsed":787,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["Loss_softmax = []\n","\n","for img in range(n_data):\n","    L_i = loss_i[img,y[img]]\n","    print('Softmax Loss for data',img,':',L_i)\n","    Loss_softmax.append(L_i)\n","\n","Loss_softmax = np.array(Loss_softmax)\n","print('\\nSoftmax Loss or Categorical Crossentropy Loss is the average of all example losses')\n","print('Softmax Loss (avg) =', np.mean(Loss_softmax))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Softmax Loss for data 0 : 0.3461525884668476\n","Softmax Loss for data 1 : 1.2782561807026984\n","Softmax Loss for data 2 : 0.7006628447549723\n","Softmax Loss for data 3 : 0.2206283114597039\n","\n","Softmax Loss or Categorical Crossentropy Loss is the average of all example losses\n","Softmax Loss (avg) = 0.6364249813460556\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oOEbAD6Y9HFb","colab_type":"code","outputId":"68756ff3-1b1e-4822-d734-90aab17fe220","executionInfo":{"status":"ok","timestamp":1567089474434,"user_tz":-420,"elapsed":780,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["Loss_natural = []\n","\n","for img in range(n_data):\n","    L_i = loss_i_natural[img,y[img]]\n","    print('Softmax Loss for data',img,':',L_i)\n","    Loss_natural.append(L_i)\n","\n","Loss_natural = np.array(Loss_natural)\n","print('\\nSoftmax Loss or Categorical Crossentropy Loss is the average of all example losses')\n","print('Softmax Natural Loss(avg) =', np.mean(Loss_natural))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Softmax Loss for data 0 : 0.7970457901050659\n","Softmax Loss for data 1 : 2.943293626713537\n","Softmax Loss for data 2 : 1.6133358215476006\n","Softmax Loss for data 3 : 0.5080154610595616\n","\n","Softmax Loss or Categorical Crossentropy Loss is the average of all example losses\n","Softmax Natural Loss(avg) = 1.4654226748564412\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0anDd3Fe9HFd","colab_type":"text"},"source":["---\n","# Comparison\n","\n","below is the comparison between 3 Losses"]},{"cell_type":"code","metadata":{"id":"ze-FSpRr9HFe","colab_type":"code","outputId":"a31526f0-acc1-4ef9-a72b-257ec1bfa1aa","executionInfo":{"status":"ok","timestamp":1567089477022,"user_tz":-420,"elapsed":781,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print('SVM Loss (avg)  =', Loss_svm, ',loss =',np.mean(Loss_svm))\n","print('Softmax Loss    =', Loss_softmax, ',loss =',np.mean(Loss_softmax))\n","print('Softmax Natural =', Loss_natural, ',loss =',np.mean(Loss_natural))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM Loss (avg)  = [0.85 6.32 2.34 0.31] ,loss = 2.455\n","Softmax Loss    = [0.35 1.28 0.7  0.22] ,loss = 0.6364249813460556\n","Softmax Natural = [0.8  2.94 1.61 0.51] ,loss = 1.4654226748564412\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MRGxW1fo9HFg","colab_type":"text"},"source":["## Practical Technique: Shift Score to reduce computation workload\n","Calculating exponent from small number is quite expensive\n","\n","shift the raw score by subtracting it with the maximum"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uH0MTkGq9HFh","colab_type":"code","outputId":"12862224-2798-4da7-9e72-b415c1358fe1","executionInfo":{"status":"ok","timestamp":1567089480353,"user_tz":-420,"elapsed":773,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["shifted_scores = scores - np.max(scores)\n","print('shifted scores')\n","print(shifted_scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shifted scores\n","[[-1.   -1.98 -1.17]\n"," [-0.97 -3.4  -1.51]\n"," [ 0.   -2.24 -1.29]\n"," [-1.84 -0.01 -0.7 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s0bYLYds9HFj","colab_type":"code","outputId":"1e64fd8d-931e-4a04-d544-af3c43f55e58","executionInfo":{"status":"ok","timestamp":1567089482976,"user_tz":-420,"elapsed":788,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print('unnormalized log probability')\n","e_shifted_scores = np.exp(shifted_scores)\n","print(e_shifted_scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["unnormalized log probability\n","[[0.37 0.14 0.31]\n"," [0.38 0.03 0.22]\n"," [1.   0.11 0.28]\n"," [0.16 0.99 0.5 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X7N8Y0A-9HFl","colab_type":"code","outputId":"8d3fed58-30af-4e89-b2ef-7810117d4b67","executionInfo":{"status":"ok","timestamp":1567089484886,"user_tz":-420,"elapsed":780,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print('normalized log probaility')\n","sum_e_shifted_score = np.sum(e_shifted_scores, axis=1, keepdims = True)\n","norm_log_prob_shifted = e_shifted_scores / sum_e_shifted_score\n","print(norm_log_prob_shifted)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["normalized log probaility\n","[[0.45 0.17 0.38]\n"," [0.6  0.05 0.35]\n"," [0.72 0.08 0.2 ]\n"," [0.1  0.6  0.3 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BhAQMLtj9HFn","colab_type":"text"},"source":["Difference between vanilla Softmax Loss and shifted Softmax Loss"]},{"cell_type":"code","metadata":{"id":"8YLNpwKS9HFo","colab_type":"code","outputId":"a44d0433-962a-4ffa-b2aa-f445607cf1c9","executionInfo":{"status":"ok","timestamp":1567089487416,"user_tz":-420,"elapsed":795,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["loss_i_shifted = -np.log10(norm_log_prob_shifted)\n","\n","Loss_shifted = []\n","\n","for img in range(n_data):\n","    L_i = loss_i_shifted[img,y[img]]\n","    print('Softmax Loss for data',img,':',L_i)\n","    Loss_shifted.append(L_i)\n","    \n","Loss_shifted = np.array(Loss_shifted)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Softmax Loss for data 0 : 0.34615258846684765\n","Softmax Loss for data 1 : 1.2782561807026986\n","Softmax Loss for data 2 : 0.7006628447549723\n","Softmax Loss for data 3 : 0.2206283114597039\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OGJrE7519HFq","colab_type":"code","outputId":"9d7b55c6-cc0a-4b46-b77c-fb01a36c2cf3","executionInfo":{"status":"ok","timestamp":1567089490293,"user_tz":-420,"elapsed":772,"user":{"displayName":"panjibagaskara","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjsfeAcI7qDbkotGQoM9YRPYdVGvO6HIK2SgGbQg=s64","userId":"16495769300327102811"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print('SVM Loss (avg)  =', Loss_svm, ',loss =',np.mean(Loss_svm))\n","print('Softmax Loss    =', Loss_softmax, ',loss =',np.mean(Loss_softmax))\n","print('Softmax Shifted =', Loss_shifted, ',loss =',np.mean(Loss_shifted))\n","print('Softmax Natural =', Loss_natural, ',loss =',np.mean(Loss_natural))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM Loss (avg)  = [0.85 6.32 2.34 0.31] ,loss = 2.455\n","Softmax Loss    = [0.35 1.28 0.7  0.22] ,loss = 0.6364249813460556\n","Softmax Shifted = [0.35 1.28 0.7  0.22] ,loss = 0.6364249813460556\n","Softmax Natural = [0.8  2.94 1.61 0.51] ,loss = 1.4654226748564412\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYqI89dWTmEX","colab_type":"text"},"source":["\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2019 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aqhn3S3Ex8me"},"source":["![footer](https://image.ibb.co/hAHDYK/footer2018.png)"]}]}